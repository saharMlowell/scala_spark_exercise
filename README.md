# scala_spark_exercise
https://ntoggle.atlassian.net/browse/PLAT-1831
1- download simple csv, name, ages, gender
2- read the file use spark, write in scala, read as data frame
3- filter (spark) on a col, make a query, find ppl less than some age. //df.filter(age < 50).select('name')
4- output as new csv file, spark
As an alternative to finding/creating a csv file the Parquet data sent previously would be a good place to start in terms of a data source
