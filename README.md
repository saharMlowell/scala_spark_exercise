# scala_spark_exercise
https://ntoggle.atlassian.net/browse/PLAT-1831<br>
1- download simple csv, name, ages, gender<br>
2- read the file use spark, write in scala, read as data frame<br>
3- filter (spark) on a col, make a query, find ppl less than some age. //df.filter(age < 50).select('name')<br>
4- output as new csv file, spark<br>
**Note**: As an alternative to finding/creating a csv file the Parquet data sent previously would be a good place to start in terms of a data source
